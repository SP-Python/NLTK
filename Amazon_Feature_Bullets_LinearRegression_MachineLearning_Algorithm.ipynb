{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io import sql\n",
    "import mysql.connector\n",
    "from mysql.connector import errorcode\n",
    "from pandas.io import sql\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "engine = create_engine('mysql+mysqlconnector://root:newrootpassword@127.0.0.1:3306/globite_amazon') #change to connect your mysql\n",
    "#if you want to append the data to an existing table\n",
    "\n",
    "sql_conn = mysql.connector.connect(user='root', password='newrootpassword',\n",
    "                                 host='127.0.0.1',\n",
    "                                 database='globite_amazon',\n",
    "                               auth_plugin='mysql_native_password')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSQL = [] #set an empty list\n",
    "cursor = sql_conn.cursor()\n",
    "sqlquery = \"SELECT distinct asin_id,brand, customer_reviews,feature_bullets FROM globite_amazon.amz_prdts where search_keyword = 'key competitors' \"\n",
    "cursor.execute(sqlquery)\n",
    "rows = cursor.fetchall()\n",
    "for row in rows:\n",
    "    dataSQL.append(list(row))\n",
    "    labels = ['asin_id','brand','customer_reviews','feature_bullets']\n",
    "    df = pd.DataFrame.from_records(dataSQL, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin_id</th>\n",
       "      <th>brand</th>\n",
       "      <th>customer_reviews</th>\n",
       "      <th>feature_bullets</th>\n",
       "      <th>Positivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00R280UQM</td>\n",
       "      <td>Shacke</td>\n",
       "      <td>6766</td>\n",
       "      <td>\" *  * Made From High Quality Water Resistant ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B017XCZY4O</td>\n",
       "      <td>BAGAIL</td>\n",
       "      <td>2586</td>\n",
       "      <td>\" *  * HIGHT QUALITY: Made of high-quality NYL...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B005LXPSFI</td>\n",
       "      <td>eBags</td>\n",
       "      <td>1471</td>\n",
       "      <td>\" *  *  * This fits your * . *  *  *  *  * Mak...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B07D25NWLK</td>\n",
       "      <td>Globite Since 1911</td>\n",
       "      <td>11</td>\n",
       "      <td>\" *  *  * This fits your * . *  *  *  *  * Mak...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B07D26SSYB</td>\n",
       "      <td>Globite Since 1911</td>\n",
       "      <td>11</td>\n",
       "      <td>\" *  *  * This fits your * . *  *  *  *  * Mak...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      asin_id               brand  customer_reviews  \\\n",
       "0  B00R280UQM              Shacke              6766   \n",
       "1  B017XCZY4O              BAGAIL              2586   \n",
       "2  B005LXPSFI               eBags              1471   \n",
       "3  B07D25NWLK  Globite Since 1911                11   \n",
       "4  B07D26SSYB  Globite Since 1911                11   \n",
       "\n",
       "                                     feature_bullets  Positivity  \n",
       "0  \" *  * Made From High Quality Water Resistant ...           1  \n",
       "1  \" *  * HIGHT QUALITY: Made of high-quality NYL...           1  \n",
       "2  \" *  *  * This fits your * . *  *  *  *  * Mak...           1  \n",
       "3  \" *  *  * This fits your * . *  *  *  *  * Mak...           0  \n",
       "4  \" *  *  * This fits your * . *  *  *  *  * Mak...           0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For our purpose, we will be focusing on customer_reviews and feature_bullets columns.\n",
    "#We then add a new column called “Positivity”, where any customer_reviews above 500 is encoded as a 1, \n",
    "#indicating it was positively rated. Otherwise, it’ll be encoded as a 0, indicating it was negatively rated.\n",
    "\n",
    "df['Positivity'] = np.where(df['customer_reviews'] > 500, 1, 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\",hight quality: made of high-quality nylon,valued &convenient:mesh top panel for easy identification of contents, and ventilationthese cubes will go the distance. no broken zippers, no weak stitching. bagail packing cubes will become your most valued and reliable travel buddy!,avoid overweight charges - your 4 piece set fits like a glove into most airline carry-on suitcases, tote, weekender, backpacks and duffel bags. great way to keep organized for family breaks, business travel, backpacking, camping, hiking, rv, cruise holidays & saddlebags. take 1 cube with you as a carry-on item to reduce weight on checked-in luggage.,extra benifit of the laundry bag:laundry bag included to help separate your dirty clothes at the end of the trip.,4 packing cubes with laundry bag- extra large( 17.5\\\\\" x 12.75\\\\\" x 4\\\\\") large (13.75\\\\\" x 12.75\\\\\" x 4\\\\\") medium (13.75x9.75x4) small (11\\\\\" x 6.75\\\\\" x 4\\\\\");\"'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let’s start by cleaning up the Text, by dropping any unnecessary symbols or words in the Text.\n",
    "## Too much messy text data.... Clean it with simple lambda functions.\n",
    "## First, to make everything lowercase, I used this:\n",
    "##The apply() method performs the specified operation on the entire feature_bullets column.\n",
    "\n",
    "df['Text'] = df.feature_bullets.apply(lambda x: x.lower())\n",
    "df['Text'] = df.Text.apply(lambda x: x.replace(' *  *  * ',''))\n",
    "df['Text'] = df.Text.apply(lambda x: x.replace(' *  * ',','))\n",
    "df['Text'] = df.Text.apply(lambda x: x.replace(' * ',''))\n",
    "#df['Text'] = df.feature_bullets1.apply(lambda x: x.replace('\",',''))\n",
    "df['Text'] = df.Text.apply(lambda x: x.replace(', * \"',''))\n",
    "#df['Text'] = df.feature_bullets1.apply(lambda x: x.replace(',\"',''))\n",
    "\n",
    "df.Text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin_id</th>\n",
       "      <th>brand</th>\n",
       "      <th>customer_reviews</th>\n",
       "      <th>feature_bullets</th>\n",
       "      <th>Positivity</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00R280UQM</td>\n",
       "      <td>Shacke</td>\n",
       "      <td>6766</td>\n",
       "      <td>\" *  * Made From High Quality Water Resistant ...</td>\n",
       "      <td>1</td>\n",
       "      <td>\",made from high quality water resistant nylon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B017XCZY4O</td>\n",
       "      <td>BAGAIL</td>\n",
       "      <td>2586</td>\n",
       "      <td>\" *  * HIGHT QUALITY: Made of high-quality NYL...</td>\n",
       "      <td>1</td>\n",
       "      <td>\",hight quality: made of high-quality nylon,va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B005LXPSFI</td>\n",
       "      <td>eBags</td>\n",
       "      <td>1471</td>\n",
       "      <td>\" *  *  * This fits your * . *  *  *  *  * Mak...</td>\n",
       "      <td>1</td>\n",
       "      <td>\"this fits your.,make sure this fits,by enteri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B07D25NWLK</td>\n",
       "      <td>Globite Since 1911</td>\n",
       "      <td>11</td>\n",
       "      <td>\" *  *  * This fits your * . *  *  *  *  * Mak...</td>\n",
       "      <td>0</td>\n",
       "      <td>\"this fits your.,make sure this fits,by enteri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B07D26SSYB</td>\n",
       "      <td>Globite Since 1911</td>\n",
       "      <td>11</td>\n",
       "      <td>\" *  *  * This fits your * . *  *  *  *  * Mak...</td>\n",
       "      <td>0</td>\n",
       "      <td>\"this fits your.,make sure this fits,by enteri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      asin_id               brand  customer_reviews  \\\n",
       "0  B00R280UQM              Shacke              6766   \n",
       "1  B017XCZY4O              BAGAIL              2586   \n",
       "2  B005LXPSFI               eBags              1471   \n",
       "3  B07D25NWLK  Globite Since 1911                11   \n",
       "4  B07D26SSYB  Globite Since 1911                11   \n",
       "\n",
       "                                     feature_bullets  Positivity  \\\n",
       "0  \" *  * Made From High Quality Water Resistant ...           1   \n",
       "1  \" *  * HIGHT QUALITY: Made of high-quality NYL...           1   \n",
       "2  \" *  *  * This fits your * . *  *  *  *  * Mak...           1   \n",
       "3  \" *  *  * This fits your * . *  *  *  *  * Mak...           0   \n",
       "4  \" *  *  * This fits your * . *  *  *  *  * Mak...           0   \n",
       "\n",
       "                                                Text  \n",
       "0  \",made from high quality water resistant nylon...  \n",
       "1  \",hight quality: made of high-quality nylon,va...  \n",
       "2  \"this fits your.,make sure this fits,by enteri...  \n",
       "3  \"this fits your.,make sure this fits,by enteri...  \n",
       "4  \"this fits your.,make sure this fits,by enteri...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "nltk.data.path.append(os.path.join(os.getcwd(), \"nltk_data\"))\n",
    "from nltk.tokenize import word_tokenize\n",
    "# List stop words\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"english\"))\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "#words = [w for w in df['Text'] if w not in stopwords.words(\"english\")]\n",
    "\n",
    "i = 0\n",
    "# Split text into words using NLTK\n",
    "df['tokenize_words']  = df['Text'].apply(word_tokenize) \n",
    "\n",
    "df['nostop_words']  = df['tokenize_words'].apply(lambda x: [item for item in x if item not in stop])\n",
    "\n",
    "df['count_nostop_words'] = df.nostop_words.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin_id</th>\n",
       "      <th>brand</th>\n",
       "      <th>customer_reviews</th>\n",
       "      <th>feature_bullets</th>\n",
       "      <th>Positivity</th>\n",
       "      <th>Text</th>\n",
       "      <th>nostop_words</th>\n",
       "      <th>tokenize_words</th>\n",
       "      <th>count_nostop_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00R280UQM</td>\n",
       "      <td>Shacke</td>\n",
       "      <td>6766</td>\n",
       "      <td>\" *  * Made From High Quality Water Resistant ...</td>\n",
       "      <td>1</td>\n",
       "      <td>\",made from high quality water resistant nylon...</td>\n",
       "      <td>[``, ,, made, high, quality, water, resistant,...</td>\n",
       "      <td>[``, ,, made, from, high, quality, water, resi...</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B017XCZY4O</td>\n",
       "      <td>BAGAIL</td>\n",
       "      <td>2586</td>\n",
       "      <td>\" *  * HIGHT QUALITY: Made of high-quality NYL...</td>\n",
       "      <td>1</td>\n",
       "      <td>\",hight quality: made of high-quality nylon,va...</td>\n",
       "      <td>[``, ,, hight, quality, :, made, high-quality,...</td>\n",
       "      <td>[``, ,, hight, quality, :, made, of, high-qual...</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B005LXPSFI</td>\n",
       "      <td>eBags</td>\n",
       "      <td>1471</td>\n",
       "      <td>\" *  *  * This fits your * . *  *  *  *  * Mak...</td>\n",
       "      <td>1</td>\n",
       "      <td>\"this fits your.,make sure this fits,by enteri...</td>\n",
       "      <td>[``, fits, your., ,, make, sure, fits, ,, ente...</td>\n",
       "      <td>[``, this, fits, your., ,, make, sure, this, f...</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B07D25NWLK</td>\n",
       "      <td>Globite Since 1911</td>\n",
       "      <td>11</td>\n",
       "      <td>\" *  *  * This fits your * . *  *  *  *  * Mak...</td>\n",
       "      <td>0</td>\n",
       "      <td>\"this fits your.,make sure this fits,by enteri...</td>\n",
       "      <td>[``, fits, your., ,, make, sure, fits, ,, ente...</td>\n",
       "      <td>[``, this, fits, your., ,, make, sure, this, f...</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B07D26SSYB</td>\n",
       "      <td>Globite Since 1911</td>\n",
       "      <td>11</td>\n",
       "      <td>\" *  *  * This fits your * . *  *  *  *  * Mak...</td>\n",
       "      <td>0</td>\n",
       "      <td>\"this fits your.,make sure this fits,by enteri...</td>\n",
       "      <td>[``, fits, your., ,, make, sure, fits, ,, ente...</td>\n",
       "      <td>[``, this, fits, your., ,, make, sure, this, f...</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      asin_id               brand  customer_reviews  \\\n",
       "0  B00R280UQM              Shacke              6766   \n",
       "1  B017XCZY4O              BAGAIL              2586   \n",
       "2  B005LXPSFI               eBags              1471   \n",
       "3  B07D25NWLK  Globite Since 1911                11   \n",
       "4  B07D26SSYB  Globite Since 1911                11   \n",
       "\n",
       "                                     feature_bullets  Positivity  \\\n",
       "0  \" *  * Made From High Quality Water Resistant ...           1   \n",
       "1  \" *  * HIGHT QUALITY: Made of high-quality NYL...           1   \n",
       "2  \" *  *  * This fits your * . *  *  *  *  * Mak...           1   \n",
       "3  \" *  *  * This fits your * . *  *  *  *  * Mak...           0   \n",
       "4  \" *  *  * This fits your * . *  *  *  *  * Mak...           0   \n",
       "\n",
       "                                                Text  \\\n",
       "0  \",made from high quality water resistant nylon...   \n",
       "1  \",hight quality: made of high-quality nylon,va...   \n",
       "2  \"this fits your.,make sure this fits,by enteri...   \n",
       "3  \"this fits your.,make sure this fits,by enteri...   \n",
       "4  \"this fits your.,make sure this fits,by enteri...   \n",
       "\n",
       "                                        nostop_words  \\\n",
       "0  [``, ,, made, high, quality, water, resistant,...   \n",
       "1  [``, ,, hight, quality, :, made, high-quality,...   \n",
       "2  [``, fits, your., ,, make, sure, fits, ,, ente...   \n",
       "3  [``, fits, your., ,, make, sure, fits, ,, ente...   \n",
       "4  [``, fits, your., ,, make, sure, fits, ,, ente...   \n",
       "\n",
       "                                      tokenize_words  count_nostop_words  \n",
       "0  [``, ,, made, from, high, quality, water, resi...                  92  \n",
       "1  [``, ,, hight, quality, :, made, of, high-qual...                 154  \n",
       "2  [``, this, fits, your., ,, make, sure, this, f...                  91  \n",
       "3  [``, this, fits, your., ,, make, sure, this, f...                 145  \n",
       "4  [``, this, fits, your., ,, make, sure, this, f...                 134  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('X', 178), ('Packing', 153), ('Cubes', 125), ('4\\\\', 76), ('Items', 75), ('Large', 70), ('Design', 62), ('Travel', 59), ('Keep', 57), ('Clothes', 55), ('Easy', 55), ('Bag', 54), ('Fits', 54), ('Laundry', 53), ('Time', 52), ('Mesh', 51), ('2', 50), ('Made', 47), ('Extra', 46), ('Luggage', 46), ('Carry-on', 45), ('Help', 43), ('Durable', 42), ('12.75\\\\', 40), ('Valued', 40), ('&', 40), ('Nylon', 39), ('Organized', 39), ('1', 39), ('Medium', 37), ('Small', 37), ('Zippers', 37), ('Bags', 34), ('Piece', 34), ('Set', 34), ('Way', 34), ('Weight', 34), ('Luggage.', 34), ('Avoid', 30), ('Convenient', 29), ('Quality', 28), ('17.5\\\\', 28), ('Stitching', 28), ('5', 28), ('Satisfaction', 28), ('Organised', 27), ('Opening', 26), ('Material', 25), ('Overweight', 25), ('Charges', 25), ('Family', 25), ('Take', 25), ('Cube', 25), ('Item', 25), ('Make', 25), ('Interior', 25), ('13.7\\\\', 24), ('Top', 23), ('Panel', 23), ('Identification', 23), ('Contents', 23), ('Globite', 23), ('Clothing', 23), ('Breathable', 23), ('Guarantee', 23), ('Place', 21), ('13.75\\\\', 20), ('13.75x9.75x4', 20), ('11\\\\', 20), ('6.75\\\\', 20), ('Included', 20), ('Separate', 20), ('Dirty', 20), ('End', 20), ('Hight', 20), ('High-quality', 20), ('Ventilationthese', 20), ('Go', 20), ('Distance', 20), ('Broken', 20), ('Weak', 20), ('Bagail', 20), ('Become', 20), ('Reliable', 20), ('Buddy', 20), ('Like', 20), ('Glove', 20), ('Airline', 20), ('Suitcases', 20), ('Tote', 20), ('Weekender', 20), ('Backpacks', 20), ('Duffel', 20), ('Great', 20), ('Breaks', 20), ('Business', 20), ('Backpacking', 20), ('Camping', 20), ('Hiking', 20), ('Rv', 20), ('Cruise', 20), ('Holidays', 20), ('Saddlebags', 20), ('Reduce', 20), ('Checked-in', 20), ('Product', 19), ('Fast', 19), ('Traveling', 19), ('Less', 19), ('Also', 18), ('Dust', 18), ('Use', 18), ('Fit', 17), ('Your.', 17), ('Sure', 17), ('Entering', 17), ('Model', 17), ('.execute', 17), ('Function', 17), ('Module', 17), ('{', 17), ('}', 17), ('Closing', 17), ('Highest', 17), ('Clean', 17), ('Sizes', 17), ('Used', 17), ('Fabric', 17), ('3', 15), ('Drawers', 14), ('Get', 14), ('Organizer', 14), ('Compartmentalizes', 14), ('Similar', 14), ('Size', 14), ('\\\\u201csmall', 14), ('Drawers\\\\u201d', 14), ('Streamline', 14), ('Efficiently', 14), ('Utilize', 14), ('Space.', 14), ('Lightweight', 14), ('Stands', 14), ('Test', 14), ('Keeping', 14), ('Secure', 14), ('Without', 14), ('Adding', 14), ('Functional', 14), ('Hectic', 14), ('Searching', 14), ('Misplaced', 14), ('Feature', 14), ('Easily', 14), ('Locate', 14), ('Suitcase', 14), ('Easy-pull', 14), ('Provide', 14), ('Reliably', 14), ('Means', 14), ('Faster', 14), ('Packing.', 14), ('Arrive', 14), ('Won\\\\u2019t', 14), ('Shift', 14), ('Transit', 14), ('Leaving', 14), ('Wrinkled', 14), ('Relax', 14), ('Create', 14), ('Personalized', 14), ('System', 14), ('Squares', 14), ('Simplify', 14), ('Work', 14), ('Trips', 14), ('Weekend', 14), ('Getaways', 14), ('Long-awaited', 14), ('Vacations', 14), (',100', 14), ('%', 14), ('Customer', 14), ('Priority', 14), ('Call', 14), ('Email', 14), ('Us', 14), ('Support', 14), ('Save', 13), ('4', 12), ('Benifit', 12), ('Trip.,4', 12), ('Bag-', 12), ('Includes', 12), ('Seams', 11), ('Finished', 11), ('Pack', 10), ('Color', 10), ('Stress', 10), ('Room', 10), ('Airport', 10), ('Stay', 9), ('Find', 9), ('Nicley', 9), ('Drawer', 9), ('Free', 9), ('Unpacking', 9), ('Hotel.', 9), ('Multi', 9), ('Travelling', 9), ('Gym', 9), ('Beach', 9), ('Even', 9), ('Home', 9), ('Shoe', 9), ('Toiletry', 9), ('Versatile', 9), ('Two-way', 9), ('Zip', 9), ('Washable', 9), ('See-through', 9), ('Panels', 9), ('Polyester', 9), ('Protection', 9), ('Soft', 9), ('Compression.', 9), ('Ifs', 9), ('Buts', 9), ('Unhappy', 9), ('Simply', 9), ('Return', 9), ('Full', 9), ('Refund', 9), ('Replacement.', 9), ('High', 8), ('Water', 8), ('Resistant', 8), ('Withstand', 8), ('Wear', 8), ('Tear', 8), ('Traveling,4', 8), ('Double', 8), ('Creates', 8), ('Strong', 8), ('Reinforced', 8), ('Lid', 8), ('Hold', 8), ('Prevents', 8), ('Middle', 8), ('Fold', 8), ('Upwards,4', 8), ('Inch', 8), ('Deep', 8), ('Allows', 8), ('Need', 8), ('Trip', 8), ('Limited', 8), ('Bags.', 8), ('Increase', 8), ('Durability.', 8), ('6', 8), ('Various', 8), ('Sizes-three', 8), ('Deifferent', 8), ('Purpose', 8), ('Tidy.,6', 8), ('Cubes:6', 8), ('2*', 8), ('2*medium', 8), ('9.8\\\\', 8), ('*slim', 8), ('X5\\\\', 8), ('X4\\\\', 8), ('Lifetime', 6), ('New', 5), ('Launch', 5), ('Sale', 5), ('Discount', 5), ('Add', 5), ('Cart', 5), ('Stuff', 5), ('Draw', 5), ('Everything', 5), ('Well', 5), ('According', 5), ('Destinations', 5), ('Weather', 5), ('Events', 5), ('Share', 5), ('Members', 5), ('His/her', 5), ('Cubes..', 5), ('Finding', 5), ('Especially', 5), ('Plane', 5), ('Catch', 5), ('Done', 5), ('Jiffy', 5), ('Different', 5), ('See', 5)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#c = Counter(df.nostop_words)\n",
    "\n",
    "c = Counter(i.capitalize() for x in df.nostop_words for i in x)\n",
    "del c['(']\n",
    "del c[')']\n",
    "del c['-']\n",
    "del c[',']\n",
    "\n",
    "\n",
    "del c[':']\n",
    "del c['!']\n",
    "del c['\\\\u2013']\n",
    "del c['``']\n",
    "del c[';']\n",
    "del c['\\\\']\n",
    "del c['.']\n",
    "del c[\"''\"]\n",
    "\n",
    "del c['Number.p.when']\n",
    "del c['Replacementpartsbulletloader\\\\']\n",
    "del c['\\\\u201d']\n",
    "del c['\\\\u2019t']\n",
    "del c['Module.initializedpx']\n",
    "#del c[\"''\"]\n",
    "#del c[\"''\"]\n",
    "\n",
    "print(c.most_common(300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train first entry: \n",
      "\n",
      " \",made from high quality water resistant nylon material to withstand the wear and tear of traveling,4 packing cubes - extra large( 17.5\\\" x 12.75\\\" x 4\\\") large (13.75\\\" x 12.75\\\" x 4\\\") medium (13.75x9.75x4) small (11\\\" x 6.75\\\" x 4\\\"),x design with double stitching \\u2013 creates a strong reinforced lid to hold your clothes in place and prevents the bag material in the middle to fold upwards,4 inch deep design allows you to fit all the clothes you need into our bags,laundry bag included to help separate your dirty clothes at the end of the trip\"\n",
      "\n",
      "\n",
      "X_train shape:  (44,)\n"
     ]
    }
   ],
   "source": [
    "#Now, let’s split our data into random training and test subsets using “Text” and “Positivity” columns, \n",
    "#and then print out the first entry and the shape of the training set.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Text'], df['Positivity'], random_state = 0)\n",
    "print('X_train first entry: \\n\\n', X_train[0])\n",
    "print('\\n\\nX_train shape: ', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at X_train, we can see that we have a collection of over 44 documents. \n",
    "# In order to perform machine learning on text documents, \n",
    "# we first need to turn these text content into numerical feature vectors that Scikit-Learn can use.\n",
    "\n",
    "\n",
    "# “bags-of-words” representation which ignores structure and simply counts how often each word occurs. \n",
    "# CountVectorizer allows us to use the bags-of-words approach, by converting a collection of text \n",
    "# documents into a matrix of token counts.\n",
    "\n",
    "# We instantiate the CountVectorizer and fit it to our training data, converting our collection of text documents \n",
    "# into a matrix of token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer().fit(X_train)\n",
    "vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The default configuration tokenizes the string, by extracting words of at least 2 letters or numbers, \n",
    "#separated by word boundaries, it then converts everything to lowercase and builds a vocabulary \n",
    "#using these tokens. We can get some of the vocabularies by using the get_feature_names method like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100',\n",
       " 'adding',\n",
       " 'as',\n",
       " 'be',\n",
       " 'buts',\n",
       " 'checkout',\n",
       " 'compression',\n",
       " 'deep',\n",
       " 'double',\n",
       " 'easily',\n",
       " 'ever',\n",
       " 'find',\n",
       " 'function',\n",
       " 'have',\n",
       " 'holidays',\n",
       " 'includes',\n",
       " 'items',\n",
       " 'lightweight',\n",
       " 'means',\n",
       " 'multi',\n",
       " 'off',\n",
       " 'own',\n",
       " 'prevents',\n",
       " 'rearrange',\n",
       " 'return',\n",
       " 'searching',\n",
       " 'simplify',\n",
       " 'squares',\n",
       " 'suitcase',\n",
       " 'the',\n",
       " 'to',\n",
       " 'trips',\n",
       " 'upwards',\n",
       " 'washable',\n",
       " 'well',\n",
       " 'wrinkled']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'apply'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-e9744afcd219>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvect1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'apply'"
     ]
    }
   ],
   "source": [
    "vect1 = CountVectorizer().apply(lambda x: [item for item in x if item not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "359"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looking at those vocabularies, we can get a small sense of what they are about . \n",
    "\n",
    "len(vect.get_feature_names())\n",
    "\n",
    "#By checking the length of get_feature_names, we can see that we’re working with 359 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, we transform the documents in X_train to a document term matrix, \n",
    "#which gives us the bags-of-word representation of X_train. \n",
    "#The result is stored in a SciPy sparse matrix, \n",
    "#where each row corresponds to a document, and each column is a word from our training vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<44x359 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 4540 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_train_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [1, 0, 0, ..., 0, 0, 1],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 1, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This interpretation of the columns can be retrieved as follows:\n",
    "X_train_vectorized.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The entries in this matrix are the number of times each word appears in each document. \n",
    "# Because the number of words in the vocabulary is so much larger than the number of words that might appear in a single text, \n",
    "# most entries of this matrix are zero.\n",
    "\n",
    "\n",
    "## Logistic Regression\n",
    "# Now, we will train the Logistic Regression classifier based on this feature matrix X_ train_ vectorized, \n",
    "# because Logistics Regression works well for high dimensional sparse data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Next, we’ll make predictions using X_test, and compute the area under the curve score.\n",
    "# Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n",
    "# \n",
    "from sklearn.metrics import roc_auc_score\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to better understand how our model makes these predictions, \n",
    "# we can use the coefficients for each feature (a word) to determine its weight in terms of positivity and negativity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs: \n",
      "['organised' 'at' 'globite' 'can' 'when' 'dust' 'also' 'use' 'keep'\n",
      " 'product' 'your' 'this' 'module' 'you' 'or' 'for' 'help' 'free' 'full'\n",
      " 'nicley']\n",
      "\n",
      "Largest Coefs: \n",
      "['and' 'to' 'the' 'packing' 'cubes' 'of' '75' '13' 'large' 'u2013']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names())\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "print('Smallest Coefs: \\n{}\\n'.format(feature_names[sorted_coef_index[:20]]))\n",
    "print('Largest Coefs: \\n{}\\n'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tf–idf term weighting\n",
    "In a large text corpus, some words will be present very often but will carry very little meaningful information \n",
    "about the actual contents of the document (such as “the”, “a” and “is”). \n",
    "If we were to feed the count data directly to a classifier those very frequent terms would shadow \n",
    "the frequencies of rarer yet more interesting terms. Tf-idf allows us to weight terms based on how \n",
    "important they are to a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import os\n",
    "DATA_DIR=\"path to data\"\n",
    "\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "import nltk\n",
    "#from spacy import English\n",
    "#Parser = English()\n",
    "from spacy.gold import GoldParse\n",
    "\n",
    "# List stop words\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"english\"))\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load the pre-trained NLP model in spacy\"\"\"\n",
    "\n",
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "\n",
    "#nlp=spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define a function to extract keywords\"\"\"\n",
    "def get_aspects(x):\n",
    "    doc=nlp(x) ## Tokenize and extract grammatical components\n",
    "    doc=[i.text for i in doc if i.text not in stop_words and i.pos_==\"NOUN\"] ## Remove common words and retain only nouns\n",
    "    doc=list(map(lambda i: i.lower(),doc)) ## Normalize text to lower case\n",
    "    #doc=pd.Series(doc)\n",
    "    doc=doc.value_counts().head().index.tolist() ## Get 5 most frequent nouns\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Apply the function to get aspects from reviews\"\"\"\n",
    "#print(get_aspects(df.feature_bullets1))\n",
    "print(get_aspects(df.feature_bullets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
