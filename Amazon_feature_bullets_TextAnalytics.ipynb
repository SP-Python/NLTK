{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io import sql\n",
    "import mysql.connector\n",
    "from mysql.connector import errorcode\n",
    "from pandas.io import sql\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "engine = create_engine('mysql+mysqlconnector://root:newrootpassword@127.0.0.1:3306/globite_amazon') #change to connect your mysql\n",
    "#if you want to append the data to an existing table\n",
    "\n",
    "sql_conn = mysql.connector.connect(user='root', password='newrootpassword',\n",
    "                                 host='127.0.0.1',\n",
    "                                 database='globite_amazon',\n",
    "                               auth_plugin='mysql_native_password')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSQL = [] #set an empty list\n",
    "cursor = sql_conn.cursor()\n",
    "sqlquery = \"SELECT distinct asin_id,brand, feature_bullets FROM globite_amazon.amz_prdts where search_keyword = 'key competitors' \"\n",
    "cursor.execute(sqlquery)\n",
    "rows = cursor.fetchall()\n",
    "for row in rows:\n",
    "    dataSQL.append(list(row))\n",
    "    labels = ['asin_id','brand','feature_bullets']\n",
    "    df = pd.DataFrame.from_records(dataSQL, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" *  * Made From High Quality Water Resistant Nylon Material to Withstand the Wear and Tear of Traveling *  * 4 Packing Cubes - Extra Large( 17.5\\\\\" x 12.75\\\\\" x 4\\\\\") Large (13.75\\\\\" x 12.75\\\\\" x 4\\\\\") Medium (13.75x9.75x4) Small (11\\\\\" x 6.75\\\\\" x 4\\\\\") *  * X Design with Double Stitching \\\\u2013 Creates a Strong Reinforced Lid to Hold Your Clothes in Place and Prevents the Bag Material in the Middle to Fold Upwards *  * 4 inch Deep Design Allows You to Fit All The Clothes You Need into Our Bags *  * Laundry Bag Included to Help Separate your Dirty Clothes At The End of The Trip *  *  * \"'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.feature_bullets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" *  * HIGHT QUALITY: Made of high-quality NYLON *  * VALUED &CONVENIENT:Mesh top panel for easy identification of contents, and ventilationThese cubes will go the distance. No broken zippers, no weak stitching. Bagail Packing Cubes will become your most valued and reliable travel buddy! *  * AVOID OVERWEIGHT CHARGES - Your 4 piece set fits like a glove into most airline carry-on suitcases, tote, weekender, backpacks and duffel bags. Great way to keep organized for FAMILY breaks, BUSINESS travel, BACKPACKING, CAMPING, HIKING, RV, CRUISE holidays & SADDLEBAGS. Take 1 cube with you as a carry-on item to reduce weight on checked-in luggage. *  * EXTRA BENIFIT of THE LAUNDRY BAG:Laundry Bag Included to Help Separate your Dirty Clothes At The End of The Trip. *  * 4 Packing Cubes With Laundry Bag- Extra Large( 17.5\\\\\" x 12.75\\\\\" x 4\\\\\") Large (13.75\\\\\" x 12.75\\\\\" x 4\\\\\") Medium (13.75x9.75x4) Small (11\\\\\" x 6.75\\\\\" x 4\\\\\"); *  *  * \"'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.feature_bullets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\",hight quality: made of high-quality nylon,valued &convenient:mesh top panel for easy identification of contents, and ventilationthese cubes will go the distance. no broken zippers, no weak stitching. bagail packing cubes will become your most valued and reliable travel buddy!,avoid overweight charges - your 4 piece set fits like a glove into most airline carry-on suitcases, tote, weekender, backpacks and duffel bags. great way to keep organized for family breaks, business travel, backpacking, camping, hiking, rv, cruise holidays & saddlebags. take 1 cube with you as a carry-on item to reduce weight on checked-in luggage.,extra benifit of the laundry bag:laundry bag included to help separate your dirty clothes at the end of the trip.,4 packing cubes with laundry bag- extra large( 17.5\\\\\" x 12.75\\\\\" x 4\\\\\") large (13.75\\\\\" x 12.75\\\\\" x 4\\\\\") medium (13.75x9.75x4) small (11\\\\\" x 6.75\\\\\" x 4\\\\\");\"'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Too much messy text data.... Clean it with simple lambda functions.\n",
    "\n",
    "## First, to make everything lowercase, I used this:\n",
    "##The apply() method performs the specified operation on the entire feature_bullets column.\n",
    "\n",
    "df.feature_bullets1 = df.feature_bullets.apply(lambda x: x.lower())\n",
    "\n",
    "df.feature_bullets1 = df.feature_bullets1.apply(lambda x: x.replace(' *  *  * ',''))\n",
    "df.feature_bullets1 = df.feature_bullets1.apply(lambda x: x.replace(' *  * ',','))\n",
    "df.feature_bullets1 = df.feature_bullets1.apply(lambda x: x.replace(' * ',''))\n",
    "#df.feature_bullets1 = df.feature_bullets1.apply(lambda x: x.replace('\",',''))\n",
    "df.feature_bullets1 = df.feature_bullets1.apply(lambda x: x.replace(', * \"',''))\n",
    "#df.feature_bullets1 = df.feature_bullets1.apply(lambda x: x.replace(',\"',''))\n",
    "\n",
    "df.feature_bullets1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\",made from high quality water resistant nylon material to withstand the wear and tear of traveling,4 packing cubes - extra large( 17.5\\\\\" x 12.75\\\\\" x 4\\\\\") large (13.75\\\\\" x 12.75\\\\\" x 4\\\\\") medium (13.75x9.75x4) small (11\\\\\" x 6.75\\\\\" x 4\\\\\"),x design with double stitching \\\\u2013 creates a strong reinforced lid to hold your clothes in place and prevents the bag material in the middle to fold upwards,4 inch deep design allows you to fit all the clothes you need into our bags,laundry bag included to help separate your dirty clothes at the end of the trip\"'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.feature_bullets1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "nltk.data.path.append(os.path.join(os.getcwd(), \"nltk_data\"))\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "i = 0\n",
    "# Split text into words using NLTK\n",
    "df['tokenize_words1']  = df.feature_bullets1.apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenize_words1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.feature_bullets[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.feature_bullets1[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train first entry: \n",
      "\n",
      " \",made from high quality water resistant nylon material to withstand the wear and tear of traveling,4 packing cubes - extra large( 17.5\\\" x 12.75\\\" x 4\\\") large (13.75\\\" x 12.75\\\" x 4\\\") medium (13.75x9.75x4) small (11\\\" x 6.75\\\" x 4\\\"),x design with double stitching \\u2013 creates a strong reinforced lid to hold your clothes in place and prevents the bag material in the middle to fold upwards,4 inch deep design allows you to fit all the clothes you need into our bags,laundry bag included to help separate your dirty clothes at the end of the trip\"\n",
      "\n",
      "\n",
      "X_train shape:  (8,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, y_train = train_test_split(df.feature_bullets1, random_state = 0)\n",
    "print('X_train first entry: \\n\\n', X_train[0])\n",
    "print('\\n\\nX_train shape: ', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#“bags-of-words” representation which ignores structure and simply counts how often each word occurs. \n",
    "#CountVectorizer allows us to use the bags-of-words approach, by converting a collection of text \n",
    "#documents into a matrix of token counts.\n",
    "\n",
    "#We instantiate the CountVectorizer and fit it to our training data, \n",
    "#converting our collection of text documents into a matrix of token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer().fit(X_train)\n",
    "vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This model has many parameters, however the default values are quite reasonable for our purpose.\n",
    "#The default configuration tokenizes the string, by extracting words of at least 2 letters or numbers, \n",
    "#separated by word boundaries, it then converts everything to lowercase and builds a vocabulary using these tokens. \n",
    "#We can get some of the vocabularies by using the get_feature_names method like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()[::2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "357"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Looking at those vocabularies, we can get a small sense of what they are about . \n",
    "#By checking the length of get_feature_names, we can see that we’re working with 29990 features.\n",
    "\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<8x357 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 844 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_train_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 8, 1, 0],\n",
       "       [0, 0, 0, ..., 9, 0, 0],\n",
       "       [0, 1, 2, ..., 3, 0, 1],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 8, 1, 0],\n",
       "       [0, 1, 2, ..., 2, 0, 0],\n",
       "       [0, 0, 0, ..., 5, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import os\n",
    "DATA_DIR=\"path to data\"\n",
    "\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "import nltk\n",
    "#from spacy import English\n",
    "#Parser = English()\n",
    "from spacy.gold import GoldParse\n",
    "\n",
    "# List stop words\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"english\"))\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load the pre-trained NLP model in spacy\"\"\"\n",
    "\n",
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "\n",
    "#nlp=spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define a function to extract keywords\"\"\"\n",
    "def get_aspects(x):\n",
    "    doc=nlp(x) ## Tokenize and extract grammatical components\n",
    "    doc=[i.text for i in doc if i.text not in stop_words and i.pos_==\"NOUN\"] ## Remove common words and retain only nouns\n",
    "    doc=list(map(lambda i: i.lower(),doc)) ## Normalize text to lower case\n",
    "    #doc=pd.Series(doc)\n",
    "    doc=doc.value_counts().head().index.tolist() ## Get 5 most frequent nouns\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument 'string' has incorrect type (expected str, got Series)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-9afb021f136f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;34m\"\"\"Apply the function to get aspects from reviews\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#print(get_aspects(df.feature_bullets1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_aspects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_bullets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-4604abb7c70d>\u001b[0m in \u001b[0;36mget_aspects\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;34m\"\"\"Define a function to extract keywords\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_aspects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdoc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m## Tokenize and extract grammatical components\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mdoc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"NOUN\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m## Remove common words and retain only nouns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdoc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m## Normalize text to lower case\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable)\u001b[0m\n\u001b[0;32m    344\u001b[0m             raise ValueError(Errors.E088.format(length=len(text),\n\u001b[0;32m    345\u001b[0m                                                 max_length=self.max_length))\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmake_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgolds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Argument 'string' has incorrect type (expected str, got Series)"
     ]
    }
   ],
   "source": [
    "\"\"\"Apply the function to get aspects from reviews\"\"\"\n",
    "#print(get_aspects(df.feature_bullets1))\n",
    "print(get_aspects(df.feature_bullets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
